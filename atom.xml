<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog of Fisher JJ</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-03-14T08:37:03.987Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Fisher JJ</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>内核传输数据弊端和解决方案</title>
    <link href="http://example.com/2024/03/14/%E5%86%85%E6%A0%B8%E4%BC%A0%E8%BE%93%E6%95%B0%E6%8D%AE%E5%BC%8A%E7%AB%AF%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>http://example.com/2024/03/14/%E5%86%85%E6%A0%B8%E4%BC%A0%E8%BE%93%E6%95%B0%E6%8D%AE%E5%BC%8A%E7%AB%AF%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2024-03-14T05:40:35.000Z</published>
    <updated>2024-03-14T08:37:03.987Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ctimbai.github.io/tags/UIO/">DPDK 入门最佳指南</a></p><h2 id="弊端"><a href="#弊端" class="headerlink" title="弊端"></a>弊端</h2><ul><li><ol><li>中断处理：当网络中大量数据包到来时，会产生频繁的硬件中断请求，这些硬件中断可以打断之前较低优先级的软中断或者系统调用的执行过程，如果这种打断频繁的话，将会产生较高的性能开销。</li></ol></li><li><ol start="2"><li>内存拷贝：正常情况下，一个网络数据包从网卡到应用程序需要经过如下的过程：数据从网卡通过dma等方式传到内核开辟的缓冲区，然后从内核空间拷贝到用户态空间，在linux内核协议中，这个耗时操作甚至占到了数据包整个处理流程的57.1%</li></ol></li><li><ol start="3"><li>上下文切换：频繁到达的硬件中断和软中断都可能随时抢占系统调用的运行，这会产生大量的上下文切换开销，另外，在基于多线程的服务器设计框架中，线程间的调度也会产生频繁的上下文切换开销，锁竞争的耗能也是十分严重的问题。</li></ol></li><li><ol start="4"><li>局部性失效：如今主流的处理器都是多个核心的，这意味着一个数据包的处理可能跨多个cpu核心，比如一个数据包可能终端在cpu0，内核态处理在 cpu1，用户态处理在 cpu2，这样跨多个核心，容易造成 CPU 缓存失效，造成局部性失效。如果是 NUMA 架构，更会造成跨 NUMA 访问内存，性能受到很大影响。</li></ol></li><li><ol start="5"><li>内存管理：传统服务器内存页为4k，为了提高内存的访问速度，避免 cache miss，可以增加 cache 中映射表的条目，但这又会影响 CPU 的检索效率。</li></ol></li></ul><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ul><li><p>1、控制层和数据层分离： 将数据包处理、内存管理、处理器调度等任务转移到用户空间去完成，而内核仅仅负责部分控制指令的处理。这样就不存在上述所说的系统中断、上下文切换、系统调用、系统调度等等问题。</p></li><li><p>2、多核技术： 使用多核编程技术代替多线程技术，并设置 CPU 的亲和性，将线程和 CPU 核进行一比一绑定，减少彼此之间调度切换。</p></li><li><p>3、NUMA 亲和性： 针对 NUMA 系统，尽量使 CPU 核使用所在 NUMA 节点的内存，避免跨内存访问。</p></li><li><p>4、大页内存： 使用大页内存代替普通的内存，减少 cache-miss。</p></li><li><p>5、无锁技术： 采用无锁技术解决资源竞争问题。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://ctimbai.github.io/tags/UIO/&quot;&gt;DPDK 入门最佳指南&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;弊端&quot;&gt;&lt;a href=&quot;#弊端&quot; class=&quot;headerlink&quot; title=&quot;弊端&quot;&gt;&lt;/a&gt;弊端&lt;/h2&gt;&lt;ul&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>uio</title>
    <link href="http://example.com/2024/03/14/uio/"/>
    <id>http://example.com/2024/03/14/uio/</id>
    <published>2024-03-14T01:33:02.000Z</published>
    <updated>2024-03-14T05:37:11.319Z</updated>
    
    <content type="html"><![CDATA[<h2 id="UIO是什么？"><a href="#UIO是什么？" class="headerlink" title="UIO是什么？"></a>UIO是什么？</h2><ul><li>UIO（Userspace i&#x2F;o）是linux 内核中的一个轻量级驱动框架，它允许用户空间程序直接访问物理设备资源，如内存，中断和DMA通道等。UIO的主要目标是提供一种简单而灵活的方式，让用户空间程序能够直接与硬件设备进行交互，而无需通过内核空间的传统驱动程序。这种模型特别适用于哪些需要高性能，低延迟或特殊硬件访问需求的场景。</li></ul><h2 id="UIO的软件架构介绍"><a href="#UIO的软件架构介绍" class="headerlink" title="UIO的软件架构介绍"></a>UIO的软件架构介绍</h2><ul><li>UIO驱动模型由内核空间的UIO核心代码和用户空间的库组成。内核空间的UIO核心代码负责设备的注册，内存映射，中断管理等操作，而用户空间的库则提供了访问这些功能模块的接口。<br><img src="/../images/uio_095113.png"></li></ul><h3 id="内核空间部分"><a href="#内核空间部分" class="headerlink" title="内核空间部分"></a>内核空间部分</h3><ul><li>UIO核心代码：这是uio驱动模型的核心，辅助设备驱动的注册和注销，内存映射的管理，中断处理等。关键函数uio_register_device()</li><li>UIO设备：UIO设备是UIO核心代码与物理设备之间的抽象层。每个UIO设备都有一个与之关联的设备文件，用户空间程序可以通过这个设备文件与设备进行通信。</li></ul><h3 id="用户空间部分："><a href="#用户空间部分：" class="headerlink" title="用户空间部分："></a>用户空间部分：</h3><ul><li>UIO用户空间库：这个库提供了一系列的API，允许用户空间程序与UIO设备进行交互，如打开设备，映射内存，处理中断等。</li><li>用户空间应用程序：这些程序使用UIO用户空间库提供的API，直接与硬件设备进行通信和控制，</li></ul><h2 id="为什么需要UIO"><a href="#为什么需要UIO" class="headerlink" title="为什么需要UIO"></a>为什么需要UIO</h2><ul><li>性能：传统linux驱动模型通常需要在内核空间和用户空间之间进行频繁的数据拷贝和上下文切换，这会导致性能下降。UIO允许用户空间程序直接访问设备资源，从而减少了内核和用户空间之间的数据拷贝和上下文切换次数，提高了性能。</li><li>灵活性：UIO驱动模型的设计非常灵活，允许开发者根据实际需求编写用户空间的驱动程序。这种灵活性使得开发者能够更容易地适应不同的硬件平台和设备需求。</li><li>简化驱动开发：UIO核心代码提供了许多通用的功能，如内存映射，中断管理等。</li></ul><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><ul><li>UIO分成两个部分：<ul><li>内核部分：主要实现硬件寄存器的内存映射（stuct uio_info -&gt; struct uio_mem）及读写操作</li><li>用户空间部分：将uio设备的uio_mem映射到本地（mmap），这样就可以实现在用户空间访问硬件设备寄存器的目的，再通过设备的控制逻辑，就可以实现硬件设备的驱动。</li></ul></li></ul><h3 id="内核部分实现原理："><a href="#内核部分实现原理：" class="headerlink" title="内核部分实现原理："></a>内核部分实现原理：</h3><ul><li>设备注册：UIO驱动首先需要在内核中注册，包括分配设备号，初始化设备结构等，</li><li>内存映射：UIO驱动通过<code>mmap</code>系统调用将物理设备 <strong>内存映射</strong> 到用户空间的虚拟地址空间。</li><li>中断管理：UIO驱动可以处理设备的中断请求，并将中断事件通知给用户空间程序。</li></ul><h3 id="用户空间部分实现原理："><a href="#用户空间部分实现原理：" class="headerlink" title="用户空间部分实现原理："></a>用户空间部分实现原理：</h3><ul><li>设备打开：用户空间程序通过打开设备文件与UIO设备建立连接。</li><li>内存映射：用户空间程序使用mmap系统调用将设备内存映射到进程的虚拟地址空间。</li><li>数据交互：用户空间程序通过读写虚拟地址的方式与设备进行数据交互。</li><li>中断处理：用户空间程序可以设置中断处理函数，以响应设备的中断事件。</li></ul><h3 id="UIO设备驱动的主要任务有两个："><a href="#UIO设备驱动的主要任务有两个：" class="headerlink" title="UIO设备驱动的主要任务有两个："></a>UIO设备驱动的主要任务有两个：</h3><ul><li>存取设备的内存uio核心实现了mmap()可以处理物理内存(physical memory)，逻辑内存（logical memory）,虚拟内存（virtual memory）.UIO驱动的编写时，就不需要再考虑这些繁琐的细节。<strong>如果有些设备的总线不是pci总线，那么仍然需要做相关处理</strong></li><li>处理设备产生的中断，对于设备中断的应答必须在内核空间进行，所以在内核空间有一小部分代码，用来应答中断和禁止中断，但是其余的工作全部给用户空间处理。</li><li>如果用户空间要等待一个设备中断，则只需要简单的阻塞在对&#x2F;dev&#x2F;uioX的操作上，当设备产生中断时，read()操作立即返回。UIO 也实现了poll()系统调用，你可以使用  select()来等待中断的发生。select()有一个超时参数可以用来实现有限时间内等待中断。   对设备的控制还可以通过&#x2F;sys&#x2F;class&#x2F;uio下的各个文件的读写来完成。你注册的uio设备将会出现在该目录下。假如你的uio设备是uio0那么映射的设备内存文件出现在 &#x2F;sys&#x2F;class&#x2F;uio&#x2F;uio0&#x2F;maps&#x2F;mapX，对该文件的读写就是对设备内存的读写。<br><img src="/../images/uio_133557.png"></li></ul><blockquote><p>UIO不是通用驱动程序接口。已经被其他内核子系统（例如网络，串行或USB）良好处理的设备不适合使用UIO驱动程序。最适合UIO驱动程序的硬件满足以下所有条件：</p><ul><li>设备具有可以映射的内存。通过写入该存储器可以完全控制该设备。</li><li>设备通常会产生中断。</li><li>该设备不适合标准内核子系统之一。</li></ul></blockquote><h2 id="UIO是怎么进行工作？"><a href="#UIO是怎么进行工作？" class="headerlink" title="UIO是怎么进行工作？"></a>UIO是怎么进行工作？</h2><ul><li><p>当使用UIO设备时，可以通过设备文件和sysfs属性文件进行访问。</p><ul><li><p>设备文件：每个UIO设备都有一个设备文件，用于访问设备的寄存器或RAM位置。第一个设备的设备文件路径是”&#x2F;dev&#x2F;uio0”，后续设备的路径依次为”&#x2F;dev&#x2F;uio1”、”&#x2F;dev&#x2F;uio2”等等。通过打开设备文件，可以使用read()和write()函数来读取和写入设备的寄存器或RAM。</p></li><li><p>sysfs属性文件：UIO设备还提供了一组sysfs属性文件，用于访问设备的状态和配置信息。这些属性文件位于”&#x2F;sys&#x2F;class&#x2F;uio&#x2F;uioX”目录下，其中X表示设备的编号。以下是一些常见的sysfs属性文件：</p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;UIO是什么？&quot;&gt;&lt;a href=&quot;#UIO是什么？&quot; class=&quot;headerlink&quot; title=&quot;UIO是什么？&quot;&gt;&lt;/a&gt;UIO是什么？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;UIO（Userspace i&amp;#x2F;o）是linux 内核中的一个轻量级驱动框架，</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>磁盘性能</title>
    <link href="http://example.com/2024/03/11/%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD/"/>
    <id>http://example.com/2024/03/11/%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD/</id>
    <published>2024-03-11T08:00:15.000Z</published>
    <updated>2024-03-11T11:09:04.240Z</updated>
    
    <content type="html"><![CDATA[<h2 id="磁盘性能指标说明"><a href="#磁盘性能指标说明" class="headerlink" title="磁盘性能指标说明"></a>磁盘性能指标说明</h2><ul><li>衡量磁盘性能常见的指标有：使用率，饱和度，iops，吞吐量以及响应时间，具体说明：<ul><li>使用率，是指磁盘处理I&#x2F;O的时间百分比。过高的使用率（比如超过80%），通常意味着磁盘I&#x2F;O存在性能瓶颈。</li><li>饱和度，是指磁盘处理I&#x2F;O的繁忙程度。过高的饱和度，意味着磁盘有严重的性能瓶颈。当饱和度为100%时，磁盘无法接受新的I&#x2F;O请求。</li><li>IOPS，<strong>是指每秒的I&#x2F;O请求数。</strong></li><li>吞吐量，是指每秒的I&#x2F;O请求大小，即每秒磁盘I&#x2F;O的流量，磁盘写入加上读出数据的大小。单位为bps。</li></ul></li></ul><blockquote><p>bps （bits per second）是数据传输速率的常用单位，意思是比特率，比特&#x2F;秒，位&#x2F;秒，每秒传的位数。<br>bps：(ps指的是&#x2F;s，即每秒)每秒钟传送多少位的信息。</p></blockquote><ul><li>响应时间，是指I&#x2F;O请求从发出到收到响应的间隔时间。</li></ul><h3 id="IOPS与吞吐量的关系"><a href="#IOPS与吞吐量的关系" class="headerlink" title="IOPS与吞吐量的关系"></a>IOPS与吞吐量的关系</h3><ul><li><p>每秒 I&#x2F;O 吞吐量＝ IOPS * 平均 I&#x2F;O SIZE。从公式可以看出： I&#x2F;O SIZE 越大，IOPS 越高，那么每秒 I&#x2F;O 的吞吐量就越高。因此，我们会认为 IOPS 和吞吐量的数值越高越好。实际上，对于一个磁盘来讲，这两个参数均有其最大值，而且这两个参数也存在着一定的关系。</p></li><li><p>吞吐量 &#x3D; IOPS * I&#x2F;O大小</p></li><li><p>从上述公式可以看出，磁盘I&#x2F;O越大，IOPS越高，那么磁盘那么每秒I&#x2F;O的吞吐量就越高，云服务器吧认为IOPS和吞吐量的数值越高越好，实际上磁盘IOPS和吞吐量两个参数是有其最大值的。</p></li><li><p>当应用的I&#x2F;O大小较大，例如离线分析、数据仓库等应用，建议您选择吞吐量更大的大数据型实例规格族。<br>当应用的I&#x2F;O对时延较为敏感，比较随机且I&#x2F;O大小相对较小，例如OLTP事务型数据库、企业级应用，如SAP等应用，建议您选择IOPS更高的ESSD云盘、SSD云盘。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;磁盘性能指标说明&quot;&gt;&lt;a href=&quot;#磁盘性能指标说明&quot; class=&quot;headerlink&quot; title=&quot;磁盘性能指标说明&quot;&gt;&lt;/a&gt;磁盘性能指标说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;衡量磁盘性能常见的指标有：使用率，饱和度，iops，吞吐量以及响应时间，具体说明</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>linux uio</title>
    <link href="http://example.com/2024/03/07/linux-uio/"/>
    <id>http://example.com/2024/03/07/linux-uio/</id>
    <published>2024-03-07T02:20:54.000Z</published>
    <updated>2024-03-08T03:23:54.653Z</updated>
    
    <content type="html"><![CDATA[<ul><li>UIO就是userspace IO，用户空间输入输出。开发者只需要在内核中写非常少量的代码（几十行），把需要哪些寄存器告诉UIO框架，UIO就会生成一个设备文件，&#x2F;dev&#x2F;uio0，然后我们只需要在用户空间打开这个文件，mmap（映射文件到内存空间），就可以直接操作寄存器了！ <strong>这样一来，即使你需要给一个复杂设备（比如摄像头）写驱动，你也可以在用户空间来反复修改、迭代你的驱动代码，根本不需要重新编译内核。非常方便。</strong></li><li>UIO的原理是，把硬件寄存器的地址用ioremap函数给映射到用户态的虚拟地址。这个过程需要修改page table，但是uio框架把这件事做好了，所以我们只需要告诉UIO我们需要哪些寄存器就好。在用户空间，我们的程序访问虚拟地址的时候，page table会把我们引导到真正的硬件的物理地址，我们就可以操纵硬件了。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;UIO就是userspace IO，用户空间输入输出。开发者只需要在内核中写非常少量的代码（几十行），把需要哪些寄存器告诉UIO框架，UIO就会生成一个设备文件，&amp;#x2F;dev&amp;#x2F;uio0，然后我们只需要在用户空间打开这个文件，mmap（映射文件到内</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>文件系统</title>
    <link href="http://example.com/2024/03/04/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    <id>http://example.com/2024/03/04/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/</id>
    <published>2024-03-04T06:56:42.000Z</published>
    <updated>2024-03-14T01:30:01.667Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文件系统的基本组成"><a href="#文件系统的基本组成" class="headerlink" title="文件系统的基本组成"></a>文件系统的基本组成</h2><blockquote><p><a href="https://www.cnblogs.com/flashsun/p/14501625.html">文件系统</a></p></blockquote><ul><li>文件系统是操作系统中负责管理持久数据的子系统<ul><li>负责把用户的文件存到磁盘硬件中。</li></ul></li><li>文件系统的基本数据单位是文件，目的是对磁盘上的文件进行组件管理，组织的方式不同，就会形成不同的文件系统。</li><li>linux中 ，一切皆文件。</li><li>linux 文件系统会为每个文件分配数据结构：索引节点（index node）和目录项（directory entry）,它们主要用来记录文件的元数据和目录层次结构。<ul><li>索引节点：也就是inode，用来记录文件的元信息，比如inode编号，文件大小，访问权限，创建时间，修改时间，<strong>数据在磁盘的位置</strong>等。索引节点是文件的唯一标识，它们之间一一对应，也同样都会被存储在硬盘中，所以<strong>索引节点同样占用磁盘空间</strong></li><li>目录项，也就是dentry, 用来记录文件的名字，索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，<strong>目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存。</strong></li></ul></li><li>索引节点唯一标识一个文件，而目录项记录着文件的名，所以目录项和索引节点的关系是多对一，也就是说，<strong>一个文件可以有多个别字。比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。</strong></li><li><strong>目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。</strong><ul><li>目录项和目录一个东西？<ul><li>目录是一个文件，持久化存储在内存</li><li>目录项是内核一个数据结构，缓存在内存。<br><img src="/../images/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_163912.png"></li></ul></li></ul></li><li>注意：块位图是管理可用的块，每一位代表一个块的使用与否。inode 位图管理的是一条一条的 inode，并不是 inode 所占用的块，比如上图中有 8 条 inode，则 inode 位图中就有 8 位是管理他们的使用与否。<br><img src="/../images/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_170935.png"></li></ul><h2 id="文件的查找和读取。"><a href="#文件的查找和读取。" class="headerlink" title="文件的查找和读取。"></a>文件的查找和读取。</h2><p>1 我们要先根据文件路径找到文件对应的inode节点。假设是个绝对路径。文件路径是&#x2F;a&#x2F;b&#x2F;c.txt。<strong>系统初始化的时候我们已经拿到了根目录对应的inode。从inode的结构体结构中，我们知道inode有一个字段保存了文件的内容。所以这时候就把根目录文件的文件内容读进来，是一系列的dir_entry结构体。然后逐个遍历，比较文件名是不是等于a，最后得到一个目录a对应的dir_entry。</strong><br>2 根据dir_entry结构体我们知道，里面不仅保存了文件名，还保存了对应的inode号。我们根据inode号把a目录文件的内容也读取进来。以此类推。最后得到c对应的dir_entry。<br>3 再根据c对应的dir_entry的inode号，从硬盘把inode的内容读进来。发现他是一个普通文件。至此，我们找到了这个文件对应的inode节点。完成fd-&gt;file结构体-&gt;inode结构体的赋值。<br>4 然后我们开始读取文件的内容。根据fd我们找到对应的inode节点。根据file结构体的pos字段，我们知道需要读取的数据在文件中的偏移。根据这个偏移，可以算出应该取i_zone[9]字段的哪个索引，文件的前7块对应索引0-6，前7到7+512对应索引7等。得到索引后，读取i_zone数组在该索引的值，即我们要读取的数据在硬盘的数据块。然后把这个数据块从硬盘读取进来。返回给用户。<br>5 至此，完成了文件的查找和读取。</p><h2 id="从Linux文件系统看文件读写过程"><a href="#从Linux文件系统看文件读写过程" class="headerlink" title="从Linux文件系统看文件读写过程"></a>从Linux文件系统看文件读写过程</h2><ul><li>提问： 在一个 txt 文件中，修改其中一个字，然后保存，这期间计算机内部到底发生了什么？<ul><li>答案：文件读写基本流程</li></ul></li></ul><h3 id="读文件"><a href="#读文件" class="headerlink" title="读文件"></a>读文件</h3><ul><li><p>进程调用库函数向内核发起读文件请求；</p></li><li><p>内核通过检查进程的文件描述符定位到虚拟文件系统的已打开文件列表表项；</p></li><li><p>调用该文件可用的系统调用函数read()</p></li><li><p>read()函数通过文件表项链接到目录项模块，根据传入的文件路径，在目录项模块中检索，找到该文件的inode；</p></li><li><p>在inode中，通过文件内容偏移量计算出要读取的页；</p></li><li><p>通过inode找到文件对应的address_space；</p></li><li><p>在address_space中访问该文件的页缓存树，查找对应的页缓存结点：</p></li><li><p>如果页缓存命中，那么直接返回文件内容；</p></li><li><p>如果页缓存缺失，那么产生一个页缺失异常，创建一个页缓存页，同时通过inode找到文件该页的磁盘地址，读取相应的页填充该缓存页；重新进行第6步查找页缓存；</p></li><li><p>文件内容读取成功。</p></li></ul><h3 id="写文件"><a href="#写文件" class="headerlink" title="写文件"></a>写文件</h3><ul><li><p>前6步和读文件一致，在address_space中查询对应页的页缓存是否存在：</p></li><li><p>如果页缓存命中，直接把文件内容修改更新在页缓存的页中。写文件就结束了。这时候文件修改位于页缓存，并没有写回到磁盘文件中去。</p></li><li><p>如果页缓存缺失，那么产生一个页缺失异常，创建一个页缓存页，同时通过inode找到文件该页的磁盘地址，读取相应的页填充该缓存页。此时缓存页命中，进行第6步。</p></li><li><p>一个页缓存中的页如果被修改，那么会被标记成脏页。脏页需要写回到磁盘中的文件块。有两种方式可以把脏页写回磁盘：</p></li><li><p>手动调用sync()或者fsync()系统调用把脏页写回<br>pdflush进程会定时把脏页写回到磁盘</p></li><li><p>同时注意，脏页不能被置换出内存，如果脏页正在被写回，那么会被设置写回标记，这时候该页就被上锁，其他写请求被阻塞直到锁释放。</p></li></ul><blockquote><p><a href="https://github.com/justtreee/blog/issues/18">从Linux文件系统看文件读写过程 </a></p></blockquote><ul><li>Linux IO 栈<br><img src="/../images/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_103452.png"><blockquote><p><a href="https://zhuanlan.zhihu.com/p/371574406">深度理解 Linux 读取文件过程！</a></p></blockquote></li></ul><h2 id="用户文件系统-FUSE"><a href="#用户文件系统-FUSE" class="headerlink" title="用户文件系统 FUSE"></a>用户文件系统 FUSE</h2><ul><li>FUSE 是filesystem in userspace的缩写，也就是用户态文件系统</li><li>FUSE 是一个用户空间文件系统的框架，这套框架包含3个组件：<ul><li>内核模块 fuser.ko: 用来接收vfs传递下来的io请求，并且把这个io封装之后，通过管道发送到用户态。</li><li>用户态lib库libfuse：解析内核态转发出来的协议包，拆解成常规的io请求</li><li>mount工具，fusemount</li></ul></li></ul><h3 id="为何需要fuse"><a href="#为何需要fuse" class="headerlink" title="为何需要fuse"></a>为何需要fuse</h3><ul><li><p>文件系统一般是实现在内核里面的，比如，Ext4、Fat32、NTFS(Kernel原生版)等常见的文件系统，其代码都在内核中，内核开发的难点在于调试和排查故障，而<strong>FUSE特殊之处就是，其文件系统的核心逻辑是在用户空间实现的。</strong></p></li><li><p>其优点是：开发调试效率高，不会出现内核态那种出现bug很容易就导致OS重启的情况。</p></li><li><p>缺点是：<strong>会损失性能</strong></p></li><li><p>fuse本质上（数据处理时）是处于现有文件系统之上的（具体实现是和现有文件系统处于同一个层次的），fuse不参与底层磁盘数据的存取，只负责处理对读取和写入的数据在逻辑上的操作而已。</p></li></ul><h3 id="FUSE流程"><a href="#FUSE流程" class="headerlink" title="FUSE流程"></a>FUSE流程</h3><ul><li>应用程序在挂载的 Fuse 的文件系统上面进行操作。</li><li>VFS 会将操作转发到 Fuse 的 kernel driver 上面。</li><li>Fuse 的 kernel driver 分配一个 request，并且将这个 request 提交到 Fuse 的 queue 上面。</li><li>Fuse 的用户态 daemon 会从 queue 里面通过读取 &#x2F;dev&#x2F;fuse 将这个 request 取出来并且处理。这里需要注意，处理 request 的时候仍然可能进入 kernel，譬如可能将 request 发到 Ext4 去实际处理。</li><li>当请求处理完毕，Daemon 会将结果写回到 &#x2F;dev&#x2F;fuse。</li><li>Fuse 的 kernel 标记这个 request 结束，然后唤醒用户应用程序。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;文件系统的基本组成&quot;&gt;&lt;a href=&quot;#文件系统的基本组成&quot; class=&quot;headerlink&quot; title=&quot;文件系统的基本组成&quot;&gt;&lt;/a&gt;文件系统的基本组成&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>二叉树，b树，b+树</title>
    <link href="http://example.com/2024/03/04/%E4%BA%8C%E5%8F%89%E6%A0%91%EF%BC%8Cb%E6%A0%91%EF%BC%8Cb-%E6%A0%91/"/>
    <id>http://example.com/2024/03/04/%E4%BA%8C%E5%8F%89%E6%A0%91%EF%BC%8Cb%E6%A0%91%EF%BC%8Cb-%E6%A0%91/</id>
    <published>2024-03-04T05:58:49.000Z</published>
    <updated>2024-03-04T05:58:49.857Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>分布式并行文件系统</title>
    <link href="http://example.com/2024/03/04/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    <id>http://example.com/2024/03/04/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/</id>
    <published>2024-03-04T01:49:52.000Z</published>
    <updated>2024-03-05T02:11:32.896Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分布式文件系统"><a href="#分布式文件系统" class="headerlink" title="分布式文件系统"></a>分布式文件系统</h2><ul><li>“分布式”是重点，它是相对于本地文件系统而言的，分布式文件系统通常指C&#x2F;S架构或网络文件系统，用户数据没有直接连接到本地主机，而是存储到远程存储服务器上。<ul><li>需要请求forwarding</li></ul></li></ul><h2 id="并行文件系统"><a href="#并行文件系统" class="headerlink" title="并行文件系统"></a>并行文件系统</h2><ul><li>支持并行应用。在并行文件系统环境下，所有客户端可以在同一时间并发读写同一个文件。<ul><li>并行文件系统，是客户端知道数据在哪一个节点上直接去对应节点获取数据，没有forwarding这个过程。</li><li>通常并行系统都是携带私有客户端</li></ul></li><li></li></ul><h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ul><li><p><strong>并行文件系统是分布式文件系统的一种</strong>。分布式文件系统和并行文件系统都可以将数据分布在多个存储服务器上，可横向扩展从而容纳PB级数据，并支持高带宽。</p></li><li><p>分布式文件系统通常也像并行文件系统一样，支持共享的全局名称空间。<strong>但是对于分布式文件系统而言，即使部分文件数据被分配和放置在不同服务器上，客户端在访问数据或元数据时，仍然需要通过指定的协议服务器来完成，协议服务器可能会成为 IO 路由的瓶颈。</strong>现在市场上主流的分布式文件存储为了解决这个协议服务器瓶颈的问题，使用 DNS 技术将不同的客户端连接至多个协议服务器，实现一定程度的负载均衡，但即使通过负载分发，也只是将不同客户端的 IO 拆分至多个协议服务器，<strong>协议服务器仍然需要进行一次 IO 转发</strong>，从而带来一定程度的性能损耗。<strong>使用并行文件系统，客户端系统可以直接访问所有存储节点以进行数据传输，而不必通过协议服务器。</strong></p></li><li><p>其他区别还包括：</p><ul><li>分布式文件系统通常使用标准的网络文件访问协议（例如 NFS 或 SMB ）来访问存储服务器。并行文件系统通常需要安装基于客户端的软件驱动程序（甚至推出基于 Windows 的客户端程序，例如 YRCloudFile、Panasas 等），通过以太网或 InfiniBand 等高速网络访问共享存储，因为只有基于这些客户端程序，才能实现区别于 NFS 的多 IO 路径访问。</li><li><strong>部分分布式文件系统将单个文件存储在单个存储节点上，而并行文件系统通常将文件分解并跨多个存储节点对数据块进行条带化。</strong></li><li>分布式文件系统倾向于带宽型或归档型应用程序。并行文件系统专注于高并发、高IOPS、海量数据的高性能工作负载。</li><li><strong>分布式文件系统通常使用诸如三副本或纠删码等技术来提供数据可靠性</strong>，而许多并行文件系统还支持后端挂载磁盘阵列（例如 Lustre、Spectrum Scale、YRCloudFile 等）。</li></ul></li><li><p>非本地直接连接，通过网络连接的，是分布式文件系统</p></li><li><p>支持并行应用的，为并行文件系统</p><ul><li>这两个概念之间具有重叠之处，比如Lustre，它既是分布式文件系统，也是并行文件系统。</li></ul></li></ul><p><a href="https://www.yanrongyun.com/zh-cn/blogs/parallel-vs-distributed-nfs-file-storage">https://www.yanrongyun.com/zh-cn/blogs/parallel-vs-distributed-nfs-file-storage</a></p><h2 id="常见的并行文件系统"><a href="#常见的并行文件系统" class="headerlink" title="常见的并行文件系统"></a>常见的并行文件系统</h2><h3 id="lustre"><a href="#lustre" class="headerlink" title="lustre"></a>lustre</h3><h4 id="lustre-是什么"><a href="#lustre-是什么" class="headerlink" title="lustre 是什么"></a>lustre 是什么</h4><ul><li>lustre 基于GNU GPL协议开源的分布式并行文件系统，主要是ddn维护。</li><li>lustre是通过内核的 lustre客户端来访问文件对象。</li></ul><h4 id="lustre-提供哪些功能"><a href="#lustre-提供哪些功能" class="headerlink" title="lustre 提供哪些功能"></a>lustre 提供哪些功能</h4><ul><li>lustre设计中最重要的是扩展性和性能。提高lustre的容量和文件系统带宽可以通过扩展更多的服务器到文件系统，通过增加lustre客户端增加并行访问lustre文件系统的性能。</li><li>lustre 文件系统提供如下的功能：<ul><li>标准的posix语义实现，lustre分布式文件系统支持ldiskfs和zfs两种文件系统<blockquote><p>当应用程序调用posix标准的接口时，posix接口会调用系统调用，此时装有lustre文件系统的操作系统会捕获这些调用，并通过Lustre文件系统的内部实现来处理这些请求。虽然从应用程序的角度看，它仍然是在调用标准的POSIX接口，但实际的底层操作是由Lustre文件系统来完成的。也就是说，Lustre作为一个兼容POSIX标准的文件系统，其目标是在不影响应用程序编程接口的前提下，优化并扩展对大规模存储系统的访问能力。应用程序无需了解或直接调用Lustre特定的接口，而是继续使用通用的POSIX系统调用接口，即可享受到Lustre带来的高性能和高可扩展性优势。</p></blockquote></li><li>自定义文件布局控制，文件系统布局决定了数据放到哪些lustre的后端服务器，文件布局可以通过lfs setstripe 来进行设置，<strong>默认的情况数据会在单个lustre后端服务器上，如果设置stripe大于1的情况，数据会被分片到多个lustre后端服务器上。</strong><ul><li>正常的文件布局是由stripe count和stripe size决定。stripe count决定使用多少个ost存储这些文件分片(stripe)，stripe size决定往下一个ost开始写之前应该往当前ost写入的数据量.</li></ul></li><li>支持高性能和大规模网络,lustre可以借助于RDMA、IB、OmniPath技术在TCP纸上提供低时延的高质量网络。<br><img src="/../images/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_135516.png"></li></ul></li></ul><h5 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h5><p><img src="/../images/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_135609.png"><br><img src="/../images/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_135625.png"><br><img src="/../images/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_135631.png"><br><img src="/../images/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_135635.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;分布式文件系统&quot;&gt;&lt;a href=&quot;#分布式文件系统&quot; class=&quot;headerlink&quot; title=&quot;分布式文件系统&quot;&gt;&lt;/a&gt;分布式文件系统&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;“分布式”是重点，它是相对于本地文件系统而言的，分布式文件系统通常指C&amp;#x2F;S架构</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>seastar</title>
    <link href="http://example.com/2024/02/29/seastar/"/>
    <id>http://example.com/2024/02/29/seastar/</id>
    <published>2024-02-29T08:26:16.000Z</published>
    <updated>2024-02-29T11:21:32.254Z</updated>
    
    <content type="html"><![CDATA[<h2 id="并发范式-share-nothing"><a href="#并发范式-share-nothing" class="headerlink" title="并发范式 share nothing"></a>并发范式 share nothing</h2><ul><li>Seastar引入了一层非常薄的抽象：它会在每个核上创建一个线程，并将此线程绑定在其上。不同核（线程）之间禁共享数据，只能通过消息队列来传递数据。</li></ul><h2 id="用户态操作系统-seastar"><a href="#用户态操作系统-seastar" class="headerlink" title="用户态操作系统 seastar"></a>用户态操作系统 seastar</h2><ul><li>Seastar 是一个应用框架，它几乎将操作系统所提供的抽象完整地搬移到了用户态中，以减少操作系统的抽象开销，实现软硬件一体化。</li></ul><h2 id="Seastar之执行流抽象"><a href="#Seastar之执行流抽象" class="headerlink" title="Seastar之执行流抽象"></a>Seastar之执行流抽象</h2><ul><li>可以将seastar想象成一个支持多核的操作系统，每个核上运行着许多的“进程”</li><li>Seastar将每个核抽象成一台单核计算机，每个单核计算机上运行着许多执行流，一个单核计算机上的多个执行流可以共享数据，不同单核计算机上的执行流只能通过消息来共享数据。</li><li>执行流：从头到尾只能在一个核上运行，并且位于不同核上执行流之间，只能通过跨核消息来通信。</li><li>Seastar在每个用户态CPU上运行一个调度器，来调度一系列的微任务。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;并发范式-share-nothing&quot;&gt;&lt;a href=&quot;#并发范式-share-nothing&quot; class=&quot;headerlink&quot; title=&quot;并发范式 share nothing&quot;&gt;&lt;/a&gt;并发范式 share nothing&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>posix</title>
    <link href="http://example.com/2024/02/29/posix/"/>
    <id>http://example.com/2024/02/29/posix/</id>
    <published>2024-02-29T02:49:25.000Z</published>
    <updated>2024-02-29T06:53:15.197Z</updated>
    
    <content type="html"><![CDATA[<h2 id="posxi-简介"><a href="#posxi-简介" class="headerlink" title="posxi 简介"></a>posxi 简介</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><ul><li>POSIX：可移植操作系统接口（Portable Operating System Interface of UNIX）,缩写为POSIX</li></ul><h3 id="发布-IEEE"><a href="#发布-IEEE" class="headerlink" title="发布-IEEE"></a>发布-IEEE</h3><h3 id="系统调用，posix-C库，系统命令和内核函数"><a href="#系统调用，posix-C库，系统命令和内核函数" class="headerlink" title="系统调用，posix, C库，系统命令和内核函数"></a>系统调用，posix, C库，系统命令和内核函数</h3><ul><li><p>linux三种接口</p><ul><li>linux提供了用户接口（shell）,库函数接口（c语言程序调用库），系统调用接口三种接口。</li></ul></li><li><p>系统调用</p><ul><li>操作系统是不能运行进程之间操作硬件资源，进程如果要想访问这些资源，必须向操作系统申请。进程向系统申请的入口就是系统调用（system call）,它由操作系统提供。</li><li>在linux中系统调用是用户空间访问内核的唯一手段，除异常和陷入外，系统调用是内核唯一的合法入口。</li></ul></li><li><p>系统调用和posix</p><ul><li>系统调用虽然是内核和用户应用程序之间的沟通桥梁，是用户应用程序访问内核的入口点。但通常，应用程序是通过操作系统提供的应用编程接口API，而不是之间通过系统调用来编程。<ul><li>应用程序可以直接调用系统调用接口，但大多数都是通过应用编程接口api。</li></ul></li><li>操作系统api 的主要功能就是把操作系统的功能完全展示出来，提供给应用程序，基于该操作系统，与文件、内存、时钟、网络、图形、各种外设等互操作的能力。此外，操作系统API通常还提供许多工具类的功能，比如操纵字符串、各种数据类型、时间日期等。 在UNIX世界里，最通用的操作系统API基于POSIX（Portable Operating System Interface of UNIX，可移植操作系统接口）标准。 </li><li>POSIX是一套操作系统接口的标准，POSIX标准定义了”POSIX兼容”的操作系统所必须提供的服务，提供了根据POSIX而定义的API 函数。这些API函数和系统调用之间有着直接的关系，<strong>一个API函数可以由一个系统调用实现，也可以通过调用多个系统调用来实现，还可以完全不使用任何系统调用。</strong></li></ul></li><li><p>系统调用和C库</p><ul><li>是 Portable Operating System Interface(可移植操作系统接口) 的缩写，X表示UNIX，它是 ISO C 的延伸，明定了一个可移植的操作系统所应具备的种种条件，其范围不只有系统函数库而已。POSIX库 就是C POSIX library。C POSIX library是C语言的POSIX系统下的标准库。包含了一些在C语言标准库之外的函数。为了OS之间的可移植性，POSIX标准规定了一些标准的接口。而这些接口标准的集合就是POSIX库。</li><li>操作系统API 通过都以c库的方式提供，linux也是如此。</li><li>C库提供了posix的绝大部分api，同时，内核提供的每个系统调用在c库中都具有相应的封装函数。</li><li>系统调用与其c库封装函数的名称常常相同，例如，read系统调用在c库中的封装函数即为read函数。</li><li>从用户的角度看，系统调用和C库之间的区别并不重要，他们只需通过C库函数完成所需功能。相反，从内核的角度看，需要考虑的则是提供哪些针对确定目的的系统调用，并不需要关注它们如何被使用。</li></ul></li><li><p>系统调用与系统命令</p><ul><li>系统命令位于C库的更上层，是利用c库实现的的可执行程序，比如最为常用的ls，cd等。</li></ul></li><li><p>系统调用和内核函数</p><ul><li>内核函数与C库函数的区别仅仅是内核函数在内核实现，因此必须遵守内核编程的规则。</li><li>系统调用最终必须具有明确的操作。</li><li>用户应用程序通过系统调用进入内核后，会执行各个系统调用对应的内核函数，</li><li>系统调用说的是操作系统提供给用户程序调用的一组“特殊”接口。用户程序可以通过这组“特殊”接口来获得操作系统内核提供的服务，比如用户可以通过文件系统相关的调用请求系统打开文件、关闭文件或读写文件，可以通过时钟相关的系统调用获得系统时间或设置系统时间等。</li><li>从逻辑上来说，系统调用可被看成是一个内核与用户空间程序交互的接口——它好比一个中间人，把用户进程的请求传达给内核，待内核把请求处理完毕后再将处理结果送回给用户空间</li><li>系统调用是用户进程进入内核的接口层，它本身并非内核函数</li><li>就像系统调用本身在应用程序和内核间的桥梁作用一样</li><li>内核提供的这组系统调用通常也被称之为系统调用接口层。系统调用接口层作为内核和用户应用程序之间的中间层，扮演了一个桥梁，或者说中间人的角色。系统调用把应用程序的请求传达给内核，待内核处理完请求后再将处理结果返回给应用程序。</li></ul></li></ul><p><img src="/../images/posix_142857.png"><br><img src="/../images/posix_142713.png"><br><img src="/../images/posix_135654.png"><br>内核函数-》系统调用函数-》C库（应用编程api posix）-》应用程序</p><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul><li><a href="https://www.cnblogs.com/invisible2/p/6871841.html">系统调用、库函数、内核函数区别</a></li><li><a href="https://blog.csdn.net/u012294613/article/details/124635425">系统调用、POSIX、C库、系统命令和内核函数</a></li><li><a href="https://www.cnblogs.com/xkfz007/articles/2176983.html">系统调用、POSIX、C库、系统命令和内核函数</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;posxi-简介&quot;&gt;&lt;a href=&quot;#posxi-简介&quot; class=&quot;headerlink&quot; title=&quot;posxi 简介&quot;&gt;&lt;/a&gt;posxi 简介&lt;/h2&gt;&lt;h3 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>bluestore</title>
    <link href="http://example.com/2024/02/29/bluestore/"/>
    <id>http://example.com/2024/02/29/bluestore/</id>
    <published>2024-02-29T02:17:07.000Z</published>
    <updated>2024-03-06T00:56:31.532Z</updated>
    
    <content type="html"><![CDATA[<h3 id="blueStore架构"><a href="#blueStore架构" class="headerlink" title="blueStore架构"></a>blueStore架构</h3><p><img src="/../images/bluestore_151131.png"></p><h3 id="BlueStore工作原理"><a href="#BlueStore工作原理" class="headerlink" title="BlueStore工作原理"></a>BlueStore工作原理</h3><ul><li><p>Ceph OSD执行两个功能</p><ul><li>在网络中和其他osd之间复制数据（多副本），</li><li>在本地存储设备上存储数据。</li></ul></li><li><p>传统的ceph osd是将数据存储到现有的文件存储模块，例如xfs文件系统，性能开销比较大，因为需要实现ceph数据到posix的转换。<br><img src="/../images/bluestore_162452.png"></p><ul><li>RocksDB：存储 WAL 、对象元数据、对象扩展属性 Omap、磁盘分配器元数据。</li><li>BlueRocksEnv：抛弃了传统文件系统，封装 RocksDB 文件操作的接口。</li><li>BlueFS：小型的 Append 文件系统，实现了 RocksDB::Env 接口，给 RocksDB 用。</li><li>Allocator：磁盘分配器，负责高效的分配磁盘空间。</li><li>根据上图，可以很直观的了解到：BlueStore 把数据分成两条路径。一条是 data 直接通过 Allocator（磁盘空间分配器）分配磁盘空间，然后写入 BlockDevice。另一条是 metadata 先写入 RocksDB（内存数据库），通过 BlueFs（BlueStore 专用文件系统）来管理 RocksDB 数据，经过 Allocator 分配磁盘空间后落入 BlockDevice。</li><li>先树立第一层的概念：BlueStore 把元数据和对象数据分开写，对象数据直接写入硬盘，而元数据则先写入超级高速的内存数据库，后续再写入稳定的硬盘设备，这个写入过程由 BlueFS 来控制。</li></ul></li></ul><h3 id="BlueStore和zns"><a href="#BlueStore和zns" class="headerlink" title="BlueStore和zns"></a>BlueStore和zns</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;blueStore架构&quot;&gt;&lt;a href=&quot;#blueStore架构&quot; class=&quot;headerlink&quot; title=&quot;blueStore架构&quot;&gt;&lt;/a&gt;blueStore架构&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/../images/bluestore_151</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>go基础</title>
    <link href="http://example.com/2024/02/19/go%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2024/02/19/go%E5%9F%BA%E7%A1%80/</id>
    <published>2024-02-19T05:16:31.000Z</published>
    <updated>2024-02-22T05:53:00.734Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>数据类型</p><ul><li>数据类型的出现是为了把数据分成所需内存大小的不同的数据。<ul><li>布尔型（bool）: 值只可以是常量true或者false</li><li>数字类型：整型int和浮点型float,支持复数，位的运算采用补码。</li><li>字符串类型（string）: 使用UTF-8编码标识Unicode文本。</li><li>其他：指针，数组，结构体（struct），联合体（union不常用），函数，切片，接口（interface）,map,channel</li></ul></li></ul></li><li><p>循环</p><ul><li>循环过程中新添加的元素是没办法遍历到的。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据类型的出现是为了把数据分成所需内存大小的不同的数据。&lt;ul&gt;
&lt;li&gt;布尔型（bool）: 值只可以是常量true或者false&lt;/li&gt;
&lt;li&gt;数字类型：整型int和浮点型float,支持复数，位的运算采用补码。</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>k8s</title>
    <link href="http://example.com/2024/01/29/k8s/"/>
    <id>http://example.com/2024/01/29/k8s/</id>
    <published>2024-01-29T07:33:08.000Z</published>
    <updated>2024-02-21T08:24:00.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="K8s简介"><a href="#K8s简介" class="headerlink" title="K8s简介"></a>K8s简介</h2><ul><li><p>k8s 是一个开放的开发平台。不局限于任何语言和任何编程接口。只要是正常的服务，都可以映射成kubernetes的Service,并通过标准的TCP通信协议进行交互。</p></li><li><p>k8s是一个完备的分布式系统支撑平台。k8s具有完备的集群管理能力，包括多层次的安全防护和准入机制，多租户应用支撑能力，透明的服务注册和服务发现机制，内建智能负载均衡器，强大的的故障发现和自我修复能力，服务滚动升级和在线扩容能力，可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。</p></li><li><p>Service 服务，是分布式集群架构的核心</p><ul><li>拥有一个唯一指定的名字</li><li>拥有一个虚拟ip（ClusterIP,Service IP,或vip（虚拟ip））和端口号</li><li>能够提供某种远程服务能力</li><li>被映射到了提供这种服务能力的一组容器应用上。</li></ul></li></ul><h2 id="基本概念和术语"><a href="#基本概念和术语" class="headerlink" title="基本概念和术语"></a>基本概念和术语</h2><ul><li><p>Node, Pod, Replication Controller, Service 等概念都可以看作一种资源对象，通过kubernetes提供的kubectl工具或者API调用进行操作，并保存在etcd中。 </p></li><li><p>Node节点是kubernetes集群中相对于Master而言的工作主机</p></li><li><p>Node可以是一台物理主机，也可以是一台虚拟机（VM）。在每个Node 上运行用于启动和管理Pod的服务-kubelet, 并能够被master管理。</p></li><li><p>Node上运行的服务进程包括kubelet,kube-proxy和docker daemon</p></li><li><p>Node的信息如下：</p><ul><li>Node地址：主机的IP地址，或者Node ID</li><li>Node运行状态：包括Pending, Runnig, Terminated三种状态。</li><li>Node Condition（条件）: 描述Running 状态node的运行条件，目前只有一种条件–Ready.<ul><li>Ready表示Node处于健康状态，可以接收从master发来的创建pod的指令。</li></ul></li><li>Node系统容量：描述Node可用的系统资源，包括cpu，内存数量，最大可调度pod的数量等。</li><li>其他：Node的其他信息，包括实例的内核版本号，kubernetes版本号，Docker版本号，操作系统名称。</li></ul></li><li><p>Node的管理</p><ul><li>Node通常是物理机，虚拟机或者云服务商提供的资源</li></ul></li><li><h2 id="使用node-controller-对node进行管理"><a href="#使用node-controller-对node进行管理" class="headerlink" title="使用node controller 对node进行管理"></a>使用node controller 对node进行管理</h2></li></ul><h3 id="pod"><a href="#pod" class="headerlink" title="pod"></a>pod</h3><ul><li>pod 是kubernetes的最基本的操作单元，<strong>包含一个或多个紧密相关的容器</strong>。</li><li>一个pod可以被一个容器化的环境看作应用层的”逻辑宿主机（logical host）”</li><li>一个pod中的多个容器应用通常是紧耦合的。</li><li>pod在node上被创建，启动或者销毁。</li><li>为什么需要pod呢？<ul><li>docker容器之间的通信受到docker网络机制的限制，在docker的世界中，一个容器需要通过link方式才能访问另一个容器提供的服务（端口），大量容器之间的link将是一件非常繁重的工作。</li><li>通过pod的概念将多个容器组合在一个虚拟机的主机内，可以实现容器之间仅需通过localhost就能相互通信了。<br><img src="/../images/k8s_155411.png"></li></ul></li><li>一个pod中的应用容器共享同一组资源，</li></ul><h2 id="基本概念和术语-1"><a href="#基本概念和术语-1" class="headerlink" title="基本概念和术语"></a>基本概念和术语</h2><h3 id="Node-（节点）"><a href="#Node-（节点）" class="headerlink" title="Node （节点）"></a>Node （节点）</h3><ul><li><p>Node（节点）是Kubernetes集群中相对于Master而言的工作主机。</p></li><li><p>Node可以是一台物理主机，也可以是一台虚拟机（VM）。</p></li><li><p>每个Node上运行用于启动和管理Pod的服务-kubelet,并能够被Master管理。</p></li><li><p><strong>在Node上运行的服务进程包括kubelet,kube-proxy和docker daemon.</strong></p></li><li><p>Node的信息：</p><ul><li>Node地址：主机的IP地址，或者Node ID</li><li>Node运行状态：包括pending, Running, Terminated三种状态</li><li>Node Condition(条件)：描述Running状态Node的运行条件，目前只有一种条件——Ready.<ul><li>Ready 表示node处于健康状态，可以接收从master发出的创建pod指令。</li></ul></li><li>Node系统容量：描述Node可用的系统资源，包括CPU，内存数量，最大可调度Pod数量等，</li><li>其他：Node的其他信息，包括实例的内核版本号，kubernetes版本号，Docker版本号，操作系统名称等，</li></ul></li><li><p>Node的管理</p><ul><li>Node通常是物理机，虚拟机或者云服务上提供的资源，并不是由kubernetes创建的。</li><li>Node Controller 对Node进行管理</li><li>Node Controller 是Kubernetes Master的一个组件，用于管理Node对象。<ul><li><ol><li>集群范围内的Node信息同步</li></ol><ul><li>可以通过启动参数设置同步的时间周期。</li></ul></li><li><ol start="2"><li>单个Node的生命周期管理</li></ol></li></ul></li></ul></li><li><p>Node的自注册</p><ul><li>当kubelet的–register-node 参数被设置为true（默认值为true）,kubelet会向apiserver注册自己。<ul><li>kubelet进行自注册的启动参数如下：<ul><li>–apiservers&#x3D;:apiserver的地址</li><li>–kubeconfig&#x3D;: 登录apiserver所需凭据&#x2F;证书的目录</li><li><ul><li></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><ul><li><p>Pod 是kubernetes的最基本的操作单元，包含一个或对个紧密相关的容器，类似于豌豆荚的概念。</p><ul><li>这些容器可以是不同类型的容器</li></ul></li><li><p>一个Pod可以被一个容器化的环境看作应用层的“逻辑宿主机”（logical host）. 一个pod中的多个容器应用通常是紧耦合的。Pod在Node上被创建，启动或者销毁。</p></li><li><p>为什么使用Pod？</p><ul><li>一个很重要的原因是：<ul><li>Docker容器之间的通信收到docker网络机制的限制，在docker的世界中，一个容器需要通过link方式才能访问另一个容器提供的服务（端口）。</li><li>大量容器之间的link将是一件非常繁重的工作。</li><li>通过pod的概念将多个容器组合在一个虚拟的“主机”内，实现了容器之间通过localhost就互相通信了。</li></ul></li></ul></li><li><p>pod, 容器，与Node之间的关系<br><img src="/../images/k8s_161508.png"></p></li><li><p>一个Pod中的应用容器共享同一组资源，如下所示：</p><ul><li>PID命令空间：Pod中的不同应用程序可以看到其他应用程序的进程ID。</li><li>网络命令空间：Pod中的多个容器能够访问同一个ip和端口范围。</li><li>IPC命令空间：Pod中的多个容器能够使用SystemV IPC 或 POSIX消息队列进行通信。</li><li>UTS命令空间：Pod中的多个容器共享一个主机名。</li><li>Volumes（共享存储卷）：Pod中的各个容器可以访问在Pod级别定义的Volumes. <ul><li>不建议在一个pod内运行相同应用的多个实例</li></ul></li></ul></li></ul><h3 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h3><ul><li><p>Replication Controller 通过lable selector 来选择要管理的pod。</p></li><li><p>在RC定义中，Pod部分的template.metadata.lables定义了Pod的label，即name&#x3D;redis-slave</p></li><li><p>在RC定义中，spec.selector中 指定name&#x3D;redis-slave,表示将所有包含该label的pod进行管理。</p></li><li><p>使用Label可以给对象创建多组标签，Service，RC等组件则通过label selector来选择对象范围，label和label selector 共同构成了kubernetes系统中最核心的应用模型，使得被管理对象能够被精细地分组管理，同时实现了整个集群的高可用性。</p></li></ul><h3 id="RC"><a href="#RC" class="headerlink" title="RC"></a>RC</h3><ul><li>Replication Cpntroller 是kubernetes 系统中的核心概念，用于定义pod副本的数量。</li><li>在master 内，controller manager 进程通过rc的定义来完成pod的创建，监控，启停等操作。</li><li>通过修改RC的副本数量，来实现Pod的动态缩放（scaling）</li></ul><h3 id="Sevice"><a href="#Sevice" class="headerlink" title="Sevice"></a>Sevice</h3><blockquote><p><a href="https://blog.csdn.net/cui_song_lin/article/details/123694590">k8s pod container node cluster 之间的关系</a></p></blockquote><h4 id="port-nodeport-targetport"><a href="#port-nodeport-targetport" class="headerlink" title="port nodeport targetport"></a>port nodeport targetport</h4><ul><li>port 是k8s 集群内部访问service的端口，即通过clusterIP:port 可以访问到某个service，仅限于集群节点内互相访问，外部无法访问。</li><li>nodeport是外部访问k8集群中service的端口，通过nodeip：nodeport可以从外部访问到某个service。</li><li>targetport 是pod的端口，从port和nodeport来的流量经过kube-proxy流入到后端pod的targetport,最后进入到容器。</li><li>containerPort<ul><li>targetPort和containerPort必须一致。</li><li>一个pod内的Container都是共享同一个Pause的命名空间的。<br><img src="/../images/k8s_151917.png"></li></ul></li></ul><h3 id="Volume-存储卷"><a href="#Volume-存储卷" class="headerlink" title="Volume (存储卷)"></a>Volume (存储卷)</h3><ul><li>Volume 是pod中能够被多个容器访问的共享目录。</li><li>k8s的volume概念与docker的volume比较类似，但不完全相同。</li><li>k8s中的volume与pod生命周期相同，但是与容器的生命周期不相关。</li><li>当容器终止或者重启时，Volume中的数据也不会丢失</li><li>k8s支持多种类型的volume，并且一个pod可以同时使用任意多个volume.</li></ul><h3 id="namespace-命名空间"><a href="#namespace-命名空间" class="headerlink" title="namespace 命名空间"></a>namespace 命名空间</h3><ul><li>namespace（命名空间）是k8s系统中的另一个非常重要的概念。<ul><li>通过将系统内部的对象“分配”到不同的namespace中，形成逻辑上分组的不同项目，小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;K8s简介&quot;&gt;&lt;a href=&quot;#K8s简介&quot; class=&quot;headerlink&quot; title=&quot;K8s简介&quot;&gt;&lt;/a&gt;K8s简介&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;k8s 是一个开放的开发平台。不局限于任何语言和任何编程接口。只要是正常的服务，都可以映射成kub</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>ubuntu 的dns 配置</title>
    <link href="http://example.com/2023/12/28/linux-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2023/12/28/linux-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/</id>
    <published>2023-12-28T08:54:47.000Z</published>
    <updated>2024-03-01T09:40:12.541Z</updated>
    
    <content type="html"><![CDATA[<h2 id="etc-resolv-conf-etc-systemd-resolved-conf-run-systemd-resolve-stub-resolv-conf-三个配置文件的区别"><a href="#etc-resolv-conf-etc-systemd-resolved-conf-run-systemd-resolve-stub-resolv-conf-三个配置文件的区别" class="headerlink" title="&#x2F;etc&#x2F;resolv.conf &#x2F;etc&#x2F;systemd&#x2F;resolved.conf &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf 三个配置文件的区别"></a>&#x2F;etc&#x2F;resolv.conf &#x2F;etc&#x2F;systemd&#x2F;resolved.conf &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf 三个配置文件的区别</h2><ul><li>&#x2F;etc&#x2F;resolv.conf 文件的传统用途是存储dns客户端的名称服务器和其他dns相关配置，如搜索域和域名缓存TTL等。然后在引入systemd-resolved服务后，DNS配置的管理发生了一些改变。<ul><li><ol><li>&#x2F;etc&#x2F;resolv.conf:</li></ol><ul><li>在未使用systemd-resolved的传统系统或已配置为直接操作此文件的系统，这是实际的dns配置文件，其中通常包含nameserver行来指定dns服务器地址</li><li>如果启用了systemd-resolved服务，&#x2F;etc&#x2F;resolv.conf往往会作为一个符号链接指向stub-resolv.conf或由systemd-resolved动态管理的虚拟文件，以确保对dns设置的更改不会被守护进程覆盖。</li></ul></li><li><ol start="2"><li>&#x2F;etc&#x2F;systemd&#x2F;resolved.conf:</li></ol><ul><li>这是 systemd-resolved 服务的配置文件，它允许用户定义持久化的 DNS 配置，包括 DNS 服务器、DNS 搜索域等参数。</li><li>修改这个文件的设置后，需要重新启动 systemd-resolved 服务以使更改生效，并且这些更改将体现在 systemd-resolved 提供给系统的 DNS 解析服务中。</li></ul></li><li><ol start="3"><li>&#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf:\</li></ol><ul><li>当启用 systemd-resolved 并采用其 DNS 解析功能时，它会在 &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf 中生成一个简化版的 DNS 配置文件（即所谓的 “stub resolver” 配置）。</li><li>&#x2F;etc&#x2F;resolv.conf 可能会指向这个文件，使得系统中的其他程序通过读取 &#x2F;etc&#x2F;resolv.conf 能够与 systemd-resolved 交互并利用其提供的 DNS 解析服务。</li><li>此文件仅包含用于连接到本地运行的 systemd-resolved 的基本信息，而具体的 DNS 服务器设置等内容则存储在 resolved.conf 中。</li></ul></li></ul></li><li>&#x2F;etc&#x2F;resolv.conf 是传统的 DNS 客户端配置入口点，但在某些系统中可能作为指向实际配置的符号链接。</li><li>&#x2F;etc&#x2F;systemd&#x2F;resolved.conf 用于配置 systemd-resolved 服务本身的全局 DNS 设置。</li><li>&#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf 由 systemd-resolved 生成，提供了一个轻量级的 DNS 客户端配置，当系统集成 systemd-resolved 时，实际的 DNS 查询工作由该服务处理。</li></ul><h3 id="系统读取顺序"><a href="#系统读取顺序" class="headerlink" title="系统读取顺序"></a>系统读取顺序</h3><ul><li><p>在启用了 systemd-resolved 的系统中，系统并不直接读取 &#x2F;etc&#x2F;systemd&#x2F;resolved.conf 和 &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf 来获取 DNS 设置。而是：</p><ul><li>systemd 初始化时会启动 systemd-resolved 服务，并加载 &#x2F;etc&#x2F;systemd&#x2F;resolved.conf 的配置，根据这里的配置设置 DNS 服务器、DNS 搜索域等参数。</li><li>systemd-resolved 依据其配置文件的内容运行，并可能会生成 &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf 文件，作为本地 DNS 解析器的配置。</li><li>系统进程 寻找 DNS 配置时，通常会首先读取 &#x2F;etc&#x2F;resolv.conf 文件。当 systemd-resolved 启用时，&#x2F;etc&#x2F;resolv.conf 通常是通过符号链接指向 &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf，这样所有需要 DNS 信息的进程都会间接使用到 systemd-resolved 提供的服务和配置。</li></ul></li><li><p>因此，从进程的角度看，它们<strong>读取的是 &#x2F;etc&#x2F;resolv.conf</strong>；而在<strong>系统初始化和服务管理层面，先读取的是 &#x2F;etc&#x2F;systemd&#x2F;resolved.conf</strong>。整个过程实际上是通过系统层级的服务管理和配置重定向来实现 DNS 配置的集中化和动态管理。</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><p>在启用 systemd-resolved 的系统中：</p><ul><li>系统首先读取 &#x2F;etc&#x2F;systemd&#x2F;resolved.conf，根据其中的配置设置 DNS 参数。</li><li>systemd-resolved 根据上述配置运行，并生成一个简化的 DNS 配置文件，例如 &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf。</li><li>为了让所有程序使用 systemd-resolved 的 DNS 服务，理想情况下应将 &#x2F;etc&#x2F;resolv.conf 设置为指向 &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;stub-resolv.conf（或其他 systemd-resolved 维护的配置文件）。</li></ul></li><li><p>如果没有做这个符号链接，程序将直接读取 &#x2F;etc&#x2F;resolv.conf 中的静态 DNS 配置，而不是 systemd-resolved 的动态配置。为解决这个问题，应创建正确的符号链接，确保所有程序都通过 &#x2F;etc&#x2F;resolv.conf 使用 systemd-resolved 的 DNS 设置。</p></li></ul><h3 id="systemd-resolved"><a href="#systemd-resolved" class="headerlink" title="systemd-resolved"></a>systemd-resolved</h3><ul><li>ubuntu中systemd-resolved服务为本地应用程序提供了网络名字解析服务，系统通过它对外进行dns请求。</li><li>系统使用systemd-resolved进行dns请求的时候，出来一些localhost等特殊域名，它需要去外部dns服务器寻找答案，这个外部dns的地址可以手动配置或者自动配置。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;etc-resolv-conf-etc-systemd-resolved-conf-run-systemd-resolve-stub-resolv-conf-三个配置文件的区别&quot;&gt;&lt;a href=&quot;#etc-resolv-conf-etc-systemd-reso</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>容器，docker</title>
    <link href="http://example.com/2023/12/20/%E5%AE%B9%E5%99%A8%EF%BC%8Cdocker/"/>
    <id>http://example.com/2023/12/20/%E5%AE%B9%E5%99%A8%EF%BC%8Cdocker/</id>
    <published>2023-12-20T08:58:21.000Z</published>
    <updated>2023-12-20T08:58:21.857Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>LDAP</title>
    <link href="http://example.com/2023/12/18/LDAP/"/>
    <id>http://example.com/2023/12/18/LDAP/</id>
    <published>2023-12-18T10:35:13.000Z</published>
    <updated>2023-12-20T11:26:44.277Z</updated>
    
    <content type="html"><![CDATA[<h2 id="LDAP-概念"><a href="#LDAP-概念" class="headerlink" title="LDAP 概念"></a>LDAP 概念</h2><ul><li>LDAP 是轻量目录访问协议，基于x.500标准，可以定制。</li><li>LDAP 支持<code>TCP/IP</code>, 这是访问internet 必须的。</li><li>LDAP是一个得到关于人或资源的集中，静态数据的快速方式。</li><li>LDAP是一个用来发布目录信息到许多不同资源的协议。</li></ul><h3 id="目录树概念"><a href="#目录树概念" class="headerlink" title="目录树概念"></a>目录树概念</h3><ul><li>目录树<ul><li>在一个目录服务系统中，整个目录信息集可以表示为一个目录信息树，树中的每个节点是一个条目（Entry）。</li></ul></li><li>条目（Entry）<ul><li>条目，也叫记录项，是LDAP中最基本的颗粒，就像字典中的词条，或者是数据库中的记录。</li><li>通过对LDAP的添加，删除，更改，检索都是以条目为基本对象的。</li></ul></li></ul><blockquote><p>LDAP 目录的条目 (entry) 由属性 (attribute) 的一个聚集组成，并由一个唯一性的名字引用，即专有名称（distinguished name，DN）。例如。DN 可以是：<br><code>&quot;cn=group,dc=eryajf,dc=net&quot;</code></p></blockquote><h4 id="对象类（ObjectClass）"><a href="#对象类（ObjectClass）" class="headerlink" title="对象类（ObjectClass）"></a>对象类（ObjectClass）</h4><ul><li><p>LDAP中，一个条目必须包含一个objectClass,且需要赋予至少一个值，每一个值将用作一条LDAP条目进行数据存储的模板。</p><ul><li>例如成员A条目，需要一个属性（是什么类的属性，例如penson类）</li></ul></li><li><p><strong>模板中包含了一个条目必须被赋值的属性和可选的属性</strong></p></li><li><p>objectClass有着严格的等级之分,最顶层是top和alias.例如,organizationalPerson这个objectClass就隶属于person,而person又隶属于top.</p></li><li><p>objectClass可分为以下3类:</p><ul><li><p>结构型(Structural):如person和organizationUnit；</p></li><li><p>辅助型(Auxiliary):如extensibeObject；</p></li><li><p>抽象型(Abstract):如top,抽象型的objectClass不能直接使用.</p></li><li><p>对象类是属性的集合，LDAP预想了很多人员组织中常见的对象，并将其封装成对象类。</p><ul><li>人员(person) 含有 含有姓（sn）、名（cn）、电话(telephoneNumber)、密码(userPassword)等属性。</li><li>单位职工(organizationalPerson)是人员(person)的继承类，除了上述属性之外还含有职务（title）、邮政编码（postalCode）、通信地址(postalAddress)等属性。</li></ul></li></ul></li></ul><h4 id="属性-Attribute"><a href="#属性-Attribute" class="headerlink" title="属性 (Attribute)"></a>属性 (Attribute)</h4><ul><li>属性(Attribute)类似于程序设计中的变量,可以被赋值.在OpenLDAP中声明了许多常用的Attribute(用户也可自己定义Attribute).</li><li>每个条目都可以有很多属性（Attribute）,比如常见的人都有姓名，地址，电话等属性。每个属性都有名称及对应的值，属性值可以有单个、多个。</li><li>属性不是随便定义的,需要符合一定的规则,而这个规则可以通过schema制定.比如,如果一个entry没有包含在 inetorgperson 这个 schema 中的objectClass: inetOrgPerson,那么就不能为它指定employeeNumber属性,因为employeeNumber是在inetOrgPerson中定义的.</li><li>常见的属性和别名<br><img src="/../images/LDAP_192439.png"></li></ul><blockquote><p>温馨提示:objectClass是一种特殊的Attribute,它包含其他用到的Attribute以及其自身.</p></blockquote><ul><li><p>对于不同的objectClass,通常具有一些必设属性值和一些可选属性值.例如,可使用person这个objectClass来表示系统中一个用户的条目,对于系统中用户通常需要有这样一些信息:姓名、电话、密码、描述等.如下图所示,对于person,通过cn和sn设置用户的名和姓,这是必须设置的,而其他属性则是可选的.</p></li><li><p>下面列出部分常用objectClass要求必设的属性.</p><ul><li>account:userid.</li><li>organization:o.</li><li>person:cn和sn.</li><li>organizationalPerson:与person相同.</li><li>organizationalRole:cn.</li><li>organizationUnit:ou.</li><li>posixGroup:cn、gidNumber.</li><li>posixAccount:cn、gidNumber、homeDirectory、uid、uidNumber.</li></ul></li></ul><h3 id="LDAP的基本模型"><a href="#LDAP的基本模型" class="headerlink" title="LDAP的基本模型"></a>LDAP的基本模型</h3><ul><li><p>LDAP的基本模型是建立在”条目“ 的基础上，一个条目是一个或多个属性的集合，并且具有一个全局唯一的”可区分名称（用dn表示）“，与关系型数据（数据库）类比，一个条目相当于数据库中的一条记录，而DN相当于数据库中记录的关键字，属性相当于数据库中的字段。</p><ul><li>一个条目是一些属性的集合，并且具有一个全局唯一的 ”可区分的名称“，为DN.<ul><li>一个条目可以通过DN来引用。</li><li>每一个条目的属性具有一个类型和一个或者多个值，类型通常是容易记忆的名称,比如”cn”是通用名称(common name),或者”mail”是电子邮件地址.</li><li>条目的值的语法取决于属性类型.比如,cn属性可能具有一个值”Babs Jensen” .一个mail属性可能包含”<a href="mailto:&#98;&#98;&#115;&#x40;&#x65;&#120;&#x61;&#109;&#112;&#x6c;&#101;&#x2e;&#x63;&#x6f;&#x6d;">&#98;&#98;&#115;&#x40;&#x65;&#120;&#x61;&#109;&#112;&#x6c;&#101;&#x2e;&#x63;&#x6f;&#x6d;</a>” .一个jpegphoto属性可能包含一幅JPEG(二进制)格式的图片.<blockquote><p>温馨提示:dn必须是全局唯一的.<br><img src="/../images/LDAP_190708.png"></p></blockquote></li></ul></li></ul></li><li><p>LDAP中，将数据组织成为一个树形结构。</p></li><li><p>,树的根结点是一个组织的域名(example.com),其下分为3个部分,分别是managers、people和group,可将这3个组看作组织中的3个部门:如managers用来管理所有管理人员,people用来管理登录系统的用户,group用来管理系统中的用户组.当然,在该图中还可继续增加其他分支.</p></li><li><p>若使用目录来保存该图中的数据,则更直观.图中每个结点用一个条目来保存,不同类型的结点需要保存的数据可能不同,<strong>在LDAP中通过一个称为objectClass的类型来控制不同结点需要的数据(称为属性).</strong></p></li><li><p>对于目录中的数据怎样进行引用呢?前面提到过,每一个条目都有一个dn,因为dn是唯一的,因此就可找到需要结点的数据.dn的构造方式如下:</p><ul><li>首先得到条目自己的名称(rdn,称为相对dn),然后开始向上逐级查找父结点,一直到根项为止.例如,对于图1-1中最右下方的结点,其dn为:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dn: cn=ldap,ou=group,o=dlw.com</span><br></pre></td></tr></table></figure></li><li>通过这样的方式,即可唯一标识每一个结点.在现实生活中,有很多这种树形结构的数据,如计算机文件系统的目录结构、Internet中的域名等.这些类型的数据,只要不需要频繁的更新,都适合用目录来保存.</li></ul></li><li><p>LDAP主要的简称含义:</p><ul><li><p>o-&gt;　organization(组织-公司)</p></li><li><p>ou-&gt;　organization unit(组织单元-部门)</p></li><li><p>c-&gt;　countryName(国家)</p></li><li><p>dc-&gt; domainComponent(域名)</p></li><li><p>sn-&gt; suer name(真实名称)</p></li><li><p>cn-&gt; common name(常用名称)</p></li><li><p><strong>Base DN:LDAP目录树的最顶部就是根,也就是所谓的“Base DN”,如”dc&#x3D;example,dc&#x3D;com”.</strong></p></li></ul></li><li><p>在LDAP中,schema用来指定一个目录中所包含的对象(objects)的类型(objectClass),以及每一个类型(objectClass)中必须提供的属性(Atrribute)和可选的属性.可将schema理解为面向对象程序设计中的类,通过类定义一个具体的对象.LDIF中的数据条目可理解为是一个具体的对象,是通过schema来规划创建的.因此,schema是一个数据模型,用来决定数据按什么方式存储,并定义存储在不同的条目(Entry)下的数据之间的关系.schema需要在主配置文件slapd.conf中指定,以用来决定在目录中可以使用哪些objectClass.</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1.  LDAP中每个条目必须属于某个或多个对象类（Object Class）</span><br><span class="line"></span><br><span class="line">2.  每个Object Class由多个属性类型组成，每个属性类型有所对应的语法和匹配规则；每个条目创建时，必须定义所属的对象类，必须提供对象类中的必选属性类型的属性值</span><br><span class="line"></span><br><span class="line">3. 例如：mail是一个属性，一个mail属性可以包括多个邮件地址，属性值是ASCII码方式的字符串，不区分大小写.</span><br><span class="line">    实际应用中一般是ldap对用户进行认证，因此对象类=person。</span><br><span class="line">    用户名属性表示用户账号的属性类型，用户名属性可以是cn或uid。</span><br><span class="line"></span><br><span class="line">4. 如果把DN看做对象的全路径，那么RDN就是其中的每一段路径。</span><br><span class="line">   通过DN的层次型语法结构，可以方便地表示出条目在LDAP树中的位置。有时在不一致引起歧义的情况下，RDN也特指DN中最靠前的一段，而剩余的部分称为父标识（Parent DN,PDN）。此处不再举例。RDN本身也可以由多个值构成，比如OU=Tech+CN=doubao,dc=shuyun,dc=com中的RDN为OU=Tech+CN=doubao，由2个值OU=Tech和CN=doubao组成，他们之间由加好隔开。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="属性详解"><a href="#属性详解" class="headerlink" title="属性详解"></a>属性详解</h4><ul><li>每个条目都可以有很多属性（Attribute）, 比如常见的人都有姓名，地址，电话等属性。</li><li>每个属性都可以名称及对应的值，属性值可以有单个，多个。</li></ul><h5 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h5><ul><li>dc (Domian componet)<ul><li>域名的部分，其格式是将完整的域名分成几部分，如域名为<code>eryajf.net</code> 变成<code>dc=eryajf,dc=net</code></li></ul></li><li>ou(organization Unit)<ul><li>组织单位，组织单位可以包含其他各种对象（包括其他组织单元）</li></ul></li><li>cn (common name)<ul><li>常用名称，可用作分组的名字，或者用户的全名。</li></ul></li><li>dn （Distinguished Name）<ul><li>每一个条目都有一个唯一的标识名，dn 在 ldap 中全局唯一，相当于该条目的唯一 ID，如上边示例中的：cn&#x3D;group,dc&#x3D;eryajf,dc&#x3D;net就是该条目的 dn。</li></ul></li><li>rdn （Relative dn）<ul><li>一般指 dn 逗号最左边的部分，如cn&#x3D;group,dc&#x3D;eryajf,dc&#x3D;net的 rdn 就是 cn&#x3D;group。</li></ul></li><li>Base DN<ul><li>LDAP 目录树的最顶部就是根，比如上边示例中的 base dn 为 dc&#x3D;eryajf,dc&#x3D;net</li></ul></li><li>description<ul><li>在不同类别中，对应不同类别的说明信息，比如用户的说明信息，分组的说明信息。</li></ul></li></ul><p><img src="/../images/LDAP_161915.png"></p><h3 id="LDAP目录结构"><a href="#LDAP目录结构" class="headerlink" title="LDAP目录结构"></a>LDAP目录结构</h3><ul><li><p>LDAP 目录以树状的层次结构来存储数据。每个目录记录有标记名（Distinguished Name，简称DN），用来读取单个记录。</p></li><li><ol><li>base dn: LDAP目录树的最顶部，也就是树的根，是上面的dc&#x3D;test,dc&#x3D;com部分，一般使用公司的域名，也可以写做o&#x3D;test.com，前者更灵活一些；</li></ol></li><li><ol start="2"><li>dc: domain component， 域名部分；</li></ol></li><li><ol start="3"><li>ou: Organization Unit，组织单位，用于将数据区分开；</li></ol></li><li><ol start="4"><li>cn：Common Name，<strong>一般使用用户名</strong>；</li></ol></li><li><ol start="5"><li>uid: 用户id，与cn的作用类型</li></ol></li><li><ol start="6"><li>sn: surname，姓</li></ol></li><li><ol start="7"><li>rdn：Relative dn，相对辨别名，类似于文件系统中的相对路径，它是与目录树结构无关的部分，如“uid&#x3D;tom”或“cn&#x3D; Thomas Johansson”（RDN之间用“逗号”隔开，它是一个键值对）</li></ol></li></ul><h3 id="LDAP属性数据库"><a href="#LDAP属性数据库" class="headerlink" title="LDAP属性数据库"></a>LDAP属性数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. dn ：一条记录的详细位置 </span><br><span class="line">2. dc ：一条记录所属区域(哪一颗树)</span><br><span class="line">3. ou ：一条记录所属组织（哪一个分支）</span><br><span class="line">4. cn/uid：一条记录的名字/ID（哪一个果实）</span><br><span class="line">5. LDAP目录树的最顶部就是根，也就是所谓的“基准DN&quot;。</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;LDAP-概念&quot;&gt;&lt;a href=&quot;#LDAP-概念&quot; class=&quot;headerlink&quot; title=&quot;LDAP 概念&quot;&gt;&lt;/a&gt;LDAP 概念&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;LDAP 是轻量目录访问协议，基于x.500标准，可以定制。&lt;/li&gt;
&lt;li&gt;LDAP</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>PCIE速率</title>
    <link href="http://example.com/2023/12/15/PCIE%E9%80%9F%E7%8E%87/"/>
    <id>http://example.com/2023/12/15/PCIE%E9%80%9F%E7%8E%87/</id>
    <published>2023-12-15T08:03:13.000Z</published>
    <updated>2024-03-11T09:32:32.269Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>PCIE的规格一般有 PCIE2.0&#x2F;PCIE3.0&#x2F;PCIE4.0&#x2F;PCIE5.0<br><a href="https://decodezp.github.io/2019/02/22/quickwords18-pcie-gtps-gbps/">几句话说清楚18：PCIE带宽单位GT&#x2F;s到Gbps转换方法</a></p></blockquote><h3 id="PCIE-相关概念"><a href="#PCIE-相关概念" class="headerlink" title="PCIE 相关概念"></a>PCIE 相关概念</h3><ul><li>传输速率为每秒传输量GT&#x2F;s, 而不是每秒位数Gbps, 因为传输率包括不提供额外吞吐量的开销位；比如PCIe 1.x 和 PCIe 2.x 使用8b&#x2F;10b 编码方案。导致占用了20%（2&#x2F;10）的原始信道带宽。</li><li>GT&#x2F;s ——Giga transation per second （千兆传输&#x2F;秒），即每一秒内传输的次数。重点在于描述物理层通信协议的速率属性，可以不和链路宽度等关联。</li><li>Gbps —— Giga Bits Per Second （千兆位&#x2F;秒）。原始速率</li><li>GT&#x2F;s 与Gbps 之间不存在成比例的换算关系。有效的带宽（乘以了比例）<br><img src="/../images/PCIE%E9%80%9F%E7%8E%87_160441.png"></li></ul><h3 id="PCIE带宽计算"><a href="#PCIE带宽计算" class="headerlink" title="PCIE带宽计算"></a>PCIE带宽计算</h3><ul><li>PCIe 吞吐量（可用带宽）计算方法：吞吐量 &#x3D; 传输速率 *  编码方案<ul><li>GT(Gigatransfer) x Lane的数量 x 编码方案效率</li><li>PCI-e2.0 协议支持 5.0 GT&#x2F;s，即每一条Lane 上支持每秒钟内传输 5G个Bit；但这并不意味着 PCIe 2.0协议的每一条Lane支持 5Gbps 的速率。为什么这么说呢？因为PCIe 2.0 的物理层协议中使用的是8b&#x2F;10b的编码方案。 即每传输8个Bit，需要发送10个Bit；这多出的2个Bit并不是对上层有意义的信息。那么， PCIe 2.0协议的每一条Lane支持 5 * 8 &#x2F; 10 &#x3D; 4 Gbps &#x3D; 500 MB&#x2F;s 的速率。以一个PCIe 2.0 x8的通道为例，x8的可用带宽为 4 * 8 &#x3D; 32 Gbps &#x3D; 4 GB&#x2F;s。同理，PCI-e3.0 协议支持 8.0 GT&#x2F;s, 即每一条Lane 上支持每秒钟内传输 8G个Bit。而PCIe 3.0 的物理层协议中使用的是 128b&#x2F;130b 的编码方案。 即每传输128个Bit，需要发送130个Bit。那么， PCIe 3.0协议的每一条Lane支持 8 * 128 &#x2F; 130 &#x3D; 7.877 Gbps &#x3D; 984.6 MB&#x2F;s 的速率。一个PCIe 3.0 x16的通道，x16 的可用带宽为7.877* 16 &#x3D; 126.031Gbps &#x3D; 15.754 GB&#x2F;s。<ul><li>PCIE速率<br><img src="/../images/PCIE%E9%80%9F%E7%8E%87_160449.png"></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;PCIE的规格一般有 PCIE2.0&amp;#x2F;PCIE3.0&amp;#x2F;PCIE4.0&amp;#x2F;PCIE5.0&lt;br&gt;&lt;a href=&quot;https://decodezp.github.io/2019/02/22/quickwords18-pci</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>nfs服务用户身份映射</title>
    <link href="http://example.com/2023/12/14/nfs%E6%9C%8D%E5%8A%A1%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E6%98%A0%E5%B0%84/"/>
    <id>http://example.com/2023/12/14/nfs%E6%9C%8D%E5%8A%A1%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E6%98%A0%E5%B0%84/</id>
    <published>2023-12-14T05:42:26.000Z</published>
    <updated>2023-12-14T07:22:20.504Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NFS-介绍"><a href="#NFS-介绍" class="headerlink" title="NFS 介绍"></a>NFS 介绍</h2><ul><li>NFS（network file system, 网络文件系统）是一种广泛的文件共享服务（企业内部网络），主要用于linux以及类unix系统之间的文件共享。</li><li>采用C&#x2F;S工作模式。在nfs服务器上将某个目录设置为共享目录，客户端可以将这个目录挂载到本地。</li><li>NFS服务本身比较简单，尤其是在权限设置方面功能比较弱。</li></ul><h3 id="用户身份映射"><a href="#用户身份映射" class="headerlink" title="用户身份映射"></a>用户身份映射</h3><ul><li>NFS服务本身并不具备身份验证功能，<strong>仅支持基于客户端IP进行认证</strong>。</li><li>我们在对NFS服务进行权限设置时，不能针对用户来分配权限，而只能针对客户端IP进行权限分配。（指的是客户端的用户，如果是truenas，则可以利用目录的权限来控制）</li><li><strong>要实现对共享目录的写入操作，必须要保证在NFS服务和操作系统两个层面全部都具有写入权限才可以。如何使得客户端可以在操作系统层面对共享目录具有写入权限，这就要涉及到用户身份映射问题。</strong></li></ul><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><ul><li><p>nfs提供了一种身份映射的机制来对用户身份进行管理。</p></li><li><p>当客户端访问nfs服务时，服务器会根据情况将客户端用户的身份映射成nfs匿名用户nfsnobody。<strong>nfsnobody是由nfs服务在系统中自动创建的一个程序用户账号</strong>，该用户不能用于登录系统，专门用作nfs服务的匿名用户账号。</p></li><li><p>用户身份映射：</p><ul><li><p>指的是当客户端访问nfs服务器时，会自动被视为服务器中的nfsnobody，并按照该用户的权限去执行操作。</p></li><li><p>但不是所有的客户端都会被映射为nfsnobody用户，在&#x2F;etc&#x2F;exports配置文件中提供了以下选项，来决定是否将nfs客户端映射为nfsnobody用户：</p><ul><li>root_squash, 当nfs客户端以root用户身份访问时，映射为nfs服务器的nfsnobody用户。（可以设置truenas中的maproot设置）</li><li>no_root_squash, 当nfs客户端以root身份访问时，映射为nfs服务器的root用户，也就是要为超级用户保留权限。这个选项会留下严重的安全隐患，一般不建议采用。</li><li>all_squash，无论NFS客户端以哪种用户身份访问，均映射为NFS服务器的nfsnobody用户。</li></ul></li><li><p>其中默认值<strong>是root_squash，即当客户端以root用户的身份访问NFS共享时，在服务器端会自动被映射为匿名账号nfsnobody。</strong></p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;NFS-介绍&quot;&gt;&lt;a href=&quot;#NFS-介绍&quot; class=&quot;headerlink&quot; title=&quot;NFS 介绍&quot;&gt;&lt;/a&gt;NFS 介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;NFS（network file system, 网络文件系统）是一种广泛的文件共享服务（企业内</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>时序数据库</title>
    <link href="http://example.com/2023/12/05/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://example.com/2023/12/05/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</id>
    <published>2023-12-05T09:28:39.000Z</published>
    <updated>2023-12-11T01:53:39.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="时序数据库"><a href="#时序数据库" class="headerlink" title="时序数据库"></a>时序数据库</h2><h3 id="时序数据库中存储的时序数据"><a href="#时序数据库中存储的时序数据" class="headerlink" title="时序数据库中存储的时序数据"></a>时序数据库中存储的时序数据</h3><ul><li><p><strong>每条数据都会带有时间戳属性。</strong></p></li><li><p>采集时序数据的目的是监测数据的前后差异，然后做出响应。</p></li><li><p>时序数据库就是把时间作为一个默认纬度罢了 一般靠谱点的话也应该以时间纬度来设计存储引擎</p></li><li><p>如果数据采集频率少，数据量不大的话，使用关系&#x2F;非关系型数据库代替时序数据库是完全没有问题的。</p></li></ul><h2 id="TDengine"><a href="#TDengine" class="headerlink" title="TDengine"></a>TDengine</h2><ul><li>TDengine是一个高效的存储、查询、分析时序大数据的平台，专为物联网、车联网、工业互联网、运维监测等优化而设计。可以像<strong>使用关系型数据库MySQL</strong>一样来使用它，简单又方便。</li><li>TDengine迅速定位了自己要做的产品，那就是物联网大数据平台，要把时序数据库、缓存、消息订阅、流式计算等系列功能融合在一起，一站式的解决物联网大数据问题，这样才能将系统研发、维护的复杂度与成本大幅降低。</li></ul><h3 id="物联网数据特点"><a href="#物联网数据特点" class="headerlink" title="物联网数据特点"></a>物联网数据特点</h3><ul><li>数据是时序的，一定带有时间戳。</li><li><strong>数据是结构化的</strong></li><li>数据极少有更新或删除操作</li><li>相对互联网应用，写多读少</li><li>用户关注的是一段时间的趋势，而不是某一特点时间点的值</li><li>数据是有保留期限的</li><li>数据的查询分析一定是基于时间段和地理区域的。</li><li>除存储查询外，还往往需要各种统计和实时计算操作</li><li>流量平稳，可以预测</li><li>往往需要插值等一些特殊的计算</li><li>数据量巨大，一天采集的数据就可以100亿条</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>物联网数据的冷热程度是时间决定的，刚采集的数据是最热的，而不是用户点击决定。</li><li>数据是时序的，时间戳自然可以作为主键，根本不需要复杂的索引结构</li></ul><h3 id="TDengine的特点"><a href="#TDengine的特点" class="headerlink" title="TDengine的特点"></a>TDengine的特点</h3><ul><li>专为物联网数据而设计，利用物联网数据时序性的特点，实现每个采集点对应一个表的这一功能。但其不适用于处理通用的互联网数据。</li><li>采用列式存储+压缩的方式，以节省硬件成本。(压缩效率高：利用物联网数据变化波动不大的特性、dif插值后压缩，然后二阶压缩，效率很高。)</li><li>支持高可用性，将每个物理节点划分为多个虚拟数据节点和虚拟管理节点。虚拟数据节点存储数据，虚拟管理节点管理MetaData。虚拟数据节点和虚拟管理节点分布在不同的物理节点上实现数据集应用的高可用。</li><li>存储结构上，采用每个采集点创建一个独立的表的方式来存储。这样实现每个采集点的数据的连续存放，提升读取效率。由于每个表的数据来源只有一个，能够实现无锁写入，提升写入速率。</li><li>针对多变的聚合，引入了超级表的概念。同一类型的采集设备可以创建一张超级表。在创建超级表时，可以对这类表指定标签，在查询的时候通过标签来对数据库中的表进行过滤，这样即使数据库中有非常多的表，也可以实现快速的多表聚合。</li><li>安装包非常小，安装使用简单。支持SQL，语法与MySQL类似。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;时序数据库&quot;&gt;&lt;a href=&quot;#时序数据库&quot; class=&quot;headerlink&quot; title=&quot;时序数据库&quot;&gt;&lt;/a&gt;时序数据库&lt;/h2&gt;&lt;h3 id=&quot;时序数据库中存储的时序数据&quot;&gt;&lt;a href=&quot;#时序数据库中存储的时序数据&quot; class=&quot;header</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>rsync</title>
    <link href="http://example.com/2023/12/01/rsync/"/>
    <id>http://example.com/2023/12/01/rsync/</id>
    <published>2023-12-01T06:10:57.000Z</published>
    <updated>2023-12-01T08:51:48.253Z</updated>
    
    <content type="html"><![CDATA[<h2 id="shell-命令行同步"><a href="#shell-命令行同步" class="headerlink" title="shell 命令行同步"></a>shell 命令行同步</h2><h3 id="ssh-同步"><a href="#ssh-同步" class="headerlink" title="ssh 同步"></a>ssh 同步</h3><ul><li>push端需要安装 rsync 服务</li><li>接收端（服务端）只需要开启 ssh服务<br><img src="/../images/rsync_153738.png"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rsync -av -e &quot;ssh -p 22&quot; /mnt/tmp/ root@10.128.118.104:/mnt/tmp</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h3 id="模组形式"><a href="#模组形式" class="headerlink" title="模组形式"></a>模组形式</h3><p><a href="https://www.cnblogs.com/jasondan/p/4040912.html">使用Rsync进行文件的同步与备份</a><br><a href="https://blog.csdn.net/scott_bing/article/details/79003688">Rsync数据同步及同步方式</a><br><img src="/../images/rsync_155349.png"></p><ul><li><p>需要在服务端配置conf文件</p><ul><li>一般位于 &#x2F;etc&#x2F;rsync.conf<br><img src="/../images/rsync_155521.png"><br>&#x2F;&#x2F; 一般加上 uid &#x3D;root<br>    gid &#x3D;root<br>    代表对同步目录的权限</li></ul></li><li><p>修改配置文件<br><code>rsync --daemon --config=/tmp/rsyncd.conf </code></p></li><li><p>推拉命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 拉文件</span><br><span class="line">rsync -av 10.128.118.229::test_bckup/ /tmp/a</span><br><span class="line"># 推文件</span><br><span class="line">rsync -av  /tmp/a/ 10.128.118.229::test_bckup/</span><br></pre></td></tr></table></figure></li></ul><h3 id="truenas-rsync-设置"><a href="#truenas-rsync-设置" class="headerlink" title="truenas rsync 设置"></a>truenas rsync 设置</h3><ul><li>架构<br><img src="/../images/rsync_163130.png"></li><li>truenas1 -&gt; (push) -&gt; truenas2</li></ul><h4 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h4><ul><li><ol><li>添加模块<br><img src="/../images/rsync_164136.png"></li></ol><ul><li>如果873是虚拟机的端口，外部网络无法直接访问，则需要通过端口映射进行映射到公网。</li></ul></li><li><p>2.<br><img src="/../images/rsync_164530.png"><br><img src="/../images/rsync_164535.png"></p><ul><li>可以设置：<ul><li>a. 最大连接数</li><li>b. 允许IP（网段）</li><li>c. 禁止ip （网段）</li></ul></li></ul></li></ul><h5 id="增加访问密码"><a href="#增加访问密码" class="headerlink" title="增加访问密码"></a>增加访问密码</h5><ul><li>添加其他参数<br><img src="/../images/rsync_164621.png"></li><li>shell 创建密码文件<br><code>vim /etc/rsync.password</code></li><li>修改权限，只有用户有读写权限<br><code>chmod 600 /etc/rsync.password</code></li></ul><h4 id="客户端-增加任务"><a href="#客户端-增加任务" class="headerlink" title="客户端 增加任务"></a>客户端 增加任务</h4><p><img src="/../images/rsync_164835.png"><br><img src="/../images/rsync_164840.png"></p><ul><li>增加用户密码文件<br><code>vim /etc/rsync.password</code></li><li>设置密码文件的拥有者 和 权限<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 /etc/rsync.password</span><br><span class="line"></span><br><span class="line">chown yujunjie:yujunjie /etc/rsync.password</span><br></pre></td></tr></table></figure></li></ul><h3 id="不配密码"><a href="#不配密码" class="headerlink" title="不配密码"></a>不配密码</h3><p><img src="/../images/rsync_165132.png"><br>&#x2F;&#x2F; 填 对该目录的 拥有者和 组就行<br><img src="/../images/rsync_165143.png"><br>&#x2F;&#x2F; 和 用户使用的密码文件 有关</p><p><a href="https://cloud.tencent.com/developer/article/1028311">Linux Rsync备份服务介绍及部署守护进程模式</a><br><a href="https://developer.aliyun.com/article/931870">超容易的rsync守护进程服务部署流程</a><br><a href="https://www.jianshu.com/p/b0157e4ab801">RSYNC备份服务</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;shell-命令行同步&quot;&gt;&lt;a href=&quot;#shell-命令行同步&quot; class=&quot;headerlink&quot; title=&quot;shell 命令行同步&quot;&gt;&lt;/a&gt;shell 命令行同步&lt;/h2&gt;&lt;h3 id=&quot;ssh-同步&quot;&gt;&lt;a href=&quot;#ssh-同步&quot; cla</summary>
      
    
    
    
    
    <category term="true nas, rsync" scheme="http://example.com/tags/true-nas-rsync/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 配置</title>
    <link href="http://example.com/2023/11/29/ubuntu-%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2023/11/29/ubuntu-%E9%85%8D%E7%BD%AE/</id>
    <published>2023-11-29T08:57:27.000Z</published>
    <updated>2023-11-29T09:03:27.919Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ubuntu-SSH-Server"><a href="#Ubuntu-SSH-Server" class="headerlink" title="Ubuntu SSH Server"></a>Ubuntu SSH Server</h2><p><a href="https://blog.csdn.net/weixin_53000184/article/details/130783369">参考</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">- 1. Ubuntu apt source configuration: </span><br><span class="line">root@ubuntu22:/mnt/nfs_dir# cat  /etc/apt/sources.list</span><br><span class="line">#deb cdrom:[Ubuntu 22.04.3 LTS _Jammy Jellyfish_ - Release amd64 (20230807.2)]/ jammy main restricted</span><br><span class="line"></span><br><span class="line"># See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to</span><br><span class="line"># newer versions of the distribution.</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted</span><br><span class="line"># deb-src http://cn.archive.ubuntu.com/ubuntu/ jammy main restricted</span><br><span class="line"></span><br><span class="line">## Major bug fix updates produced after the final release of the</span><br><span class="line">## distribution.</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted</span><br><span class="line"># deb-src http://cn.archive.ubuntu.com/ubuntu/ jammy-updates main restricted</span><br><span class="line"></span><br><span class="line">- 2. Install openssh server:  apt install openssh-server </span><br><span class="line">- 3. Enable openssh </span><br><span class="line">root@ubuntu22:/mnt/nfs_dir# cat /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line">#LoginGraceTime 2m</span><br><span class="line">PermitRootLogin prohibit-password</span><br><span class="line">#StrictModes yes</span><br><span class="line">#MaxAuthTries 6</span><br><span class="line">#MaxSessions 10</span><br><span class="line"></span><br><span class="line">PubkeyAuthentication yes</span><br><span class="line">    </span><br><span class="line">- 4. Start ssh server: #service ssh  restart </span><br><span class="line">- 5. Nfs mount: </span><br><span class="line">apt install nfs-common</span><br><span class="line">mount -t nfs -o vers=4,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport  10.128.118.104:/mnt/FileDir/iso_nfs_dir /mnt/nfs_dir</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/../images/ubuntu-%E9%85%8D%E7%BD%AE_170327.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Ubuntu-SSH-Server&quot;&gt;&lt;a href=&quot;#Ubuntu-SSH-Server&quot; class=&quot;headerlink&quot; title=&quot;Ubuntu SSH Server&quot;&gt;&lt;/a&gt;Ubuntu SSH Server&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;h</summary>
      
    
    
    
    
  </entry>
  
</feed>
